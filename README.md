# llama.cpp-Launcher

## Description

This is a tool which run **llama.cpp** quickly and conveniently on `Windows` and `Linux`.

## How to Use

1. install [llama.cpp](https://github.com/ggml-org/llama.cpp/releases).
2. set your `SERVER_DIR` and `MODEL_PATH`.
3. run the batchfile.
4. you can also set parameter (`NUM_THREADS`, `GPU_LAYERS`, etc.) according to your CPU threads and GPU VRAM size.

## llama.cpp document

### Build

[docs/build.md](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md)

### llama-cli(Local Chat)

1. [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/tools/main/README.md)
2. [ik_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp/blob/main/examples/main/README.md)

### LLaMA.cpp HTTP Server(API Server)

1. [llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md)
2. [ik_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp/blob/main/examples/server/README.md)
